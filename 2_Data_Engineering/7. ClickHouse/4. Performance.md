# Why ClickHouse is So Fast ðŸš€  
*The secret sauce behind blazing-fast analytics at scale*

Ever wonder how **ClickHouse** manages to crunch billions of rows in milliseconds?  
Itâ€™s not magicâ€”itâ€™s **brilliant engineering**. ClickHouse was designed from scratch to be **fast, efficient, and scalable** for analytical workloads.  

Hereâ€™s how it pulls it off:  

---

## 1. Concurrent Inserts Are Isolated from Each Other  
- Each insert writes to its own **data part**â€”no conflicts, no waiting.  
- Inserts donâ€™t immediately merge into the main table. Instead:  
  - Data is written into new part files.  
  - Later, a background merge process integrates them.  
  - Once merged, the temporary part is deleted.  

âœ… Multiple inserts can happen **at the same time**â€”no locks, no slowdowns.  
ðŸ‘‰ Think of it like students dropping assignments into **separate inbox trays**.  

ðŸ“˜ Learn more: [Concurrent Inserts Are Isolated](https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/mergetree)  

---

## 2. Inserts and SELECTs Donâ€™t Block Each Other  
- Reads (SELECTs) and writes (INSERTs) run **in parallel**.  
- ClickHouse uses **separate thread pools** for:  
  - Inserts  
  - Merges  
  - SELECT queries  

âœ… Like **different lanes on a highway**, operations flow independently.  
ðŸ‘‰ This means queries stay fast, even while new data is streaming in.  

ðŸ“˜ Learn more: [Concurrent Inserts and SELECTs Are Isolated](https://clickhouse.com/docs/en/operations/settings/settings#background-pool-size)  

---

## 3. Merge-Time Computation (Not Insert-Time)  
- Unlike other databases, ClickHouse **doesnâ€™t slow down inserts** by deduplicating or aggregating immediately.  
- Heavy operations are done during the **merge phase**:  
  - **Replacing merges** â†’ keep the latest row version.  
  - **Aggregating merges** â†’ combine pre-aggregated values.  
  - **TTL merges** â†’ delete or compress old data.  

âœ… Inserts stay **lightweight and fast**.  
ðŸ‘‰ Optimization happens later in the background.  

ðŸ“˜ Learn more: [Merge-Time Computation](https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/mergetree)  

---

## 4. Data Compression Saves Space and Boosts Speed  
- ClickHouse uses specialized **compression codecs**:  
  - **Delta / DoubleDelta** â†’ for sequences of numbers.  
  - **GCD** â†’ for values with common divisors.  
  - **Gorilla** â†’ for time-series data.  

âœ… Compressed data = **less storage + faster queries**.  
ðŸ‘‰ See your detailed guide here: [02_data_compression.md](./02_data_compression.md)  

ðŸ“˜ Learn more: [Data Compression](https://clickhouse.com/docs/en/sql-reference/statements/create/table#codecs)  

---

## 5. Vectorized Query Processing = All Cores, All In  
- Processes data in **batches**, not row by row.  
- Uses **SIMD (Single Instruction, Multiple Data)** to fully utilize the CPU.  
- Distributes chunks of data across all available CPU cores in parallel.  

âœ… Faster CPU cache use  
âœ… Full CPU core parallelization  
âœ… Hardware-optimized instructions  

ðŸ‘‰ If one server canâ€™t handle it, scale out with **multiple ClickHouse nodes**.  
ClickHouse scales **vertically (cores)** and **horizontally (nodes)**.  

ðŸ“˜ Learn more: [Query Processing Layer](https://clickhouse.com/docs/en/development/architecture)  

---

## 6. Data Pruning: Skip the Irrelevant  
- ClickHouse avoids reading data it doesnâ€™t need by skipping **granules**.  
- A **granule = 8192 rows** with metadata describing whatâ€™s inside.  

### Example:  
```sql
SELECT * FROM books WHERE genre = 'Fantasy';
```
- Instead of scanning all rows:
  - Looks at **metadata** of each granule.
  - Skips any granule without "Fantasy".
âœ… Skipping even 1 granule = skipping **8192 rows** at once.
ðŸ‘‰ Across millions of rows, thatâ€™s a massive speedup.
ðŸ“˜ Learn more: [Data Pruning](https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/mergetree#data-skipping-indexes)
