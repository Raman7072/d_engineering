# ðŸ“Š From Data to Insight: Building Your First ETL Pipeline with Weather Data

This guide helps beginners understand how raw weather data is transformed into visual insights using ETL (Extract, Transform, Load) processes. You'll build a complete pipeline using Python, SQLite, and matplotlib, with explanations, diagrams, and exercises.

---

## âœ… **1. What is ETL?**

**ETL** stands for:

1. **Extract** â†’ Pull data from sources like APIs, CSV files, or databases.
2. **Transform** â†’ Clean, reshape, and summarize data.
3. **Load** â†’ Store or visualize the transformed data.

### ðŸ“– **Analogy: Making Chai**
- Extract â†’ Water, milk, tea leaves
- Transform â†’ Boiling and straining
- Load â†’ Pour into a cup for drinking

---

## âœ… **2. Whatâ€™s a Pipeline?**

A **pipeline** is a sequence of connected steps that data travels through.

### Data Flow Diagram
```mermaid
flowchart LR
1[Extract] --> 2[Transform]
2 --> 3[Load]
```
---

### ðŸ“– **Analogy: Morning Routine**
- Wake up â†’ Brush teeth â†’ Grab coffee  
Just like this, data follows a structured flow to become usable.

---

## âœ… **3. Real-World Use Cases**

| Domain            | Example                         |
|-----------------|---------------------------------|
| Weather          | Hourly weather â†’ Forecasts      |
| E-commerce       | Sales â†’ Daily/Monthly reports   |
| Healthcare       | Patient stats â†’ Health trends   |

**Insight:** ETL pipelines help organizations turn raw data into decisions.

---

## âœ… **4. Tech Stack: From Simple to Industrial**

| Stage        | Beginner Tools        | Industry Tools              |
|--------------|----------------------|----------------------------|
| Extract      | Python requests, CSV | Kafka, CDC                 |
| Transform    | pandas, SQLite, SQL | Apache Spark, dbt          |
| Load         | matplotlib, SQLite  | PostgreSQL, lakehouse     |
| Orchestration| Python scripts       | Airflow, Dagster, Prefect |

---

## âœ… **5. Why Start with Python, SQLite & Matplotlib?**

- **Python** â†’ Easy to fetch and process data
- **SQLite** â†’ Simple local database, no server setup
- **matplotlib** â†’ Quick visualization with minimal code

Starting small ensures you learn the basics before handling large-scale systems.

---

## âœ… **6. Weather Data ETL Project**

### ðŸ”¢ **Step-by-Step Implementation**

### âž¤ **Extract: Reading & Fetching Data**

#### Example Code â€“ Reading Cities

```python
import pandas as pd

cities = pd.read_csv('indian_cities.csv')
print(cities.head())
```
#### Example Code â€“ Fetching Weather Data
```python
import requests
import time

API_KEY = "your_api_key"
BASE_URL = "http://api.weatherapi.com/v1/current.json"

def fetch_weather(city):
    params = {
        'key': API_KEY,
        'q': city,
        'aqi': 'no'
    }
    response = requests.get(BASE_URL, params=params)
    return response.json()

for city in cities['name']:
    data = fetch_weather(city)
    print(data['current']['temp_c'], data['current']['humidity'])
    time.sleep(8)
```
---

### âž¤ Transform: Aggregating Data
#### Example Code â€“ Daily Average Calculation
```python
import sqlite3

conn = sqlite3.connect('weather.db')
cursor = conn.cursor()

# Example: create tables
cursor.execute('''CREATE TABLE IF NOT EXISTS weather (
    city TEXT,
    temperature REAL,
    humidity REAL,
    timestamp TEXT
)''')

# Calculate daily average
query = '''
    INSERT INTO daily_weather(city, avg_temp, avg_humidity, date)
    SELECT city, AVG(temperature), AVG(humidity), DATE(timestamp)
    FROM weather
    GROUP BY city, DATE(timestamp)
'''
cursor.execute(query)
conn.commit()
```

### âž¤ Load: Visualizing the Data
#### Example Code â€“ Plotting Temperature Trends
```python
import matplotlib.pyplot as plt

data = pd.read_sql_query("SELECT * FROM daily_weather", conn)

for city in data['city'].unique():
    city_data = data[data['city'] == city]
    plt.plot(city_data['date'], city_data['avg_temp'], label=city)

plt.xlabel("Date")
plt.ylabel("Average Temperature (Â°C)")
plt.title("Daily Temperature Trends")
plt.legend()
plt.show()
```
---

## âœ… 7. Diagrams for Better Understanding
#### Data Flow
```mermaid
flowchart TD
A[Cities CSV] --> B[Extractor]
            B --> C[Weather Database]
            B --> D[Transformer]
            D --> E[Visualizer]
```
---

## âœ… 8. Exercises
#### Exercise 1 â€“ Extend the Extraction
- Add more fields from the API response (e.g., wind speed, pressure).
- Store them in the `weather` table.

#### Exercise 2 â€“ Advanced Transformation
- Calculate temperature variance per city.
- Visualize it alongside averages.

#### Exercise 3 â€“ Explore Orchestration
- Write a script that schedules API requests every hour.
- Learn about cron jobs or Python's `schedule` library.

#### Exercise 4 â€“ Experiment with New Tools
- Try using PostgreSQL instead of SQLite.
- Explore how pandas can handle larger datasets more efficiently.
---

## âœ… 9. Final Thoughts
By completing this guide, you will:
    âœ” Understand the core concepts of ETL
    âœ” Build an end-to-end pipeline from scratch
    âœ” Learn how to transform messy data into clear insights
    âœ” Gain the confidence to scale to industrial tools like Spark, Airflow, and more

---

ðŸ“‚ Ready to build your own ETL pipeline? Start with Python, SQLite, and matplotlib, and you'll soon be processing data like a pro!
